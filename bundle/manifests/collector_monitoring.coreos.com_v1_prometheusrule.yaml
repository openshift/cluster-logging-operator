apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector
spec:
  groups:
  - name: logging_collector.alerts
    rules:
    - alert: CollectorNodeDown
      annotations:
        message: Prometheus could not scrape {{ $labels.namespace }}/{{ $labels.pod
          }} collector component for more than 10m.
        summary: Collector cannot be scraped
      expr: |
        up{app_kubernetes_io_component = "collector", app_kubernetes_io_part_of = "cluster-logging"} == 0
      for: 10m
      labels:
        service: collector
        severity: critical
    - alert: CollectorHighErrorRate
      annotations:
        message: '{{ $value }}% of records have resulted in an error by {{ $labels.namespace
          }}/{{ $labels.pod }} collector component.'
        summary: '{{ $labels.namespace }}/{{ $labels.pod }} collector component errors
          are high'
      expr: |
        100 * (
            collector:log_num_errors:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          /
            collector:received_events:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          ) > 0.001
      for: 15m
      labels:
        service: collector
        severity: critical
    - alert: CollectorVeryHighErrorRate
      annotations:
        message: '{{ $value }}% of records have resulted in an error by {{ $labels.namespace
          }}/{{ $labels.pod }} collector component.'
        summary: '{{ $labels.namespace }}/{{ $labels.pod }} collector component errors
          are very high'
      expr: |
        100 * (
            collector:log_num_errors:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          /
            collector:received_events:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          ) > 0.05
      for: 15m
      labels:
        service: collector
        severity: critical
    - alert: FluentdQueueLengthIncreasing
      annotations:
        message: For the last hour, fluentd {{ $labels.pod }} output '{{ $labels.plugin_id
          }}' average buffer queue length has increased continuously.
        summary: Fluentd pod {{ $labels.pod }} is unable to keep up with traffic over
          time for forwarder output {{ $labels.plugin_id }}.
      expr: |
        sum by (pod,plugin_id) ( 0 * (deriv(fluentd_output_status_emit_records[1m] offset 1h)))  + on(pod,plugin_id)  ( deriv(fluentd_output_status_buffer_queue_length[10m]) > 0 and delta(fluentd_output_status_buffer_queue_length[1h]) > 1 )
      for: 1h
      labels:
        service: collector
        severity: Warning
    - alert: ElasticsearchDeprecation
      annotations:
        message: The OpenShift Elasticsearch Operator is deprecated and is planned
          to be removed in a future release. Red Hat provides bug fixes and support
          for this feature during the current release lifecycle, but this feature
          no longer receives enhancements. As an alternative to using the OpenShift
          Elasticsearch Operator to manage the default log storage, you can use the
          Loki Operator.
        summary: Detected Elasticsearch as the in-cluster storage which is deprecated
          and will be removed in a future release.
      expr: |
        sum(kube_pod_labels{namespace="openshift-logging",label_component='elasticsearch'}) > 0
      for: 5m
      labels:
        service: storage
        severity: Warning
    - alert: FluentdDeprecation
      annotations:
        message: Fluentd is deprecated and is planned to be removed in a future release.
          Red Hat provides bug fixes and support for this feature during the current
          release lifecycle, but this feature no longer receives enhancements. As
          an alternative to Fluentd, you can use Vector instead.
        summary: Detected Fluentd as the collector which is deprecated and will be
          removed in a future release.
      expr: |
        sum(kube_pod_labels{namespace="openshift-logging", label_implementation='fluentd', label_app_kubernetes_io_managed_by="cluster-logging-operator"}) > 0
      for: 5m
      labels:
        service: collector
        severity: Warning
    - alert: KibanaDeprecation
      annotations:
        message: The Kibana web console is now deprecated and is planned to be removed
          in a future logging release.
        summary: Detected Kibana as the visualization which is deprecated and will
          be removed in a future release.
      expr: |
        sum(kube_pod_labels{namespace="openshift-logging",label_component='kibana'}) > 0
      for: 5m
      labels:
        service: visualization
        severity: Warning
    - alert: DiskBufferUsage
      annotations:
        message: 'Collectors potentially consuming too much node disk, {{ $value }}% '
        summary: Detected consuming too much node disk on $labels.hostname host
      expr: "(label_replace(sum by(hostname) (vector_buffer_byte_size{component_kind='sink',
        buffer_type='disk'}), 'instance', '$1', 'hostname', '(.*)') \n/ on(instance)
        group_left() sum by(instance) (node_filesystem_size_bytes{mountpoint='/var'}))
        * 100  > 15\n"
      for: 5m
      labels:
        service: collector
        severity: Warning
  - name: logging_clusterlogging_telemetry.rules
    rules:
    - expr: |
        sum by(cluster)(log_collected_bytes_total)
      record: cluster:log_collected_bytes_total:sum
    - expr: |
        sum by(cluster)(log_logged_bytes_total)
      record: cluster:log_logged_bytes_total:sum
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_part_of)(rate(vector_component_errors_total[2m])) or sum by(pod, namespace, app_kubernetes_io_part_of)(rate(fluentd_output_status_num_errors[2m]))
      record: collector:log_num_errors:sum_rate
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_part_of)(rate(vector_component_received_events_total[2m])) or sum by(pod, namespace, app_kubernetes_io_part_of)(rate(fluentd_output_status_emit_records[2m]))
      record: collector:received_events:sum_rate
