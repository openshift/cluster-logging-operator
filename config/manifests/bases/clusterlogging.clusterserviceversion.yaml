apiVersion: operators.coreos.com/v1alpha1
kind: ClusterServiceVersion
metadata:
  annotations:
    capabilities: Seamless Upgrades
    categories: OpenShift Optional, Logging & Tracing
    certified: "false"
    console.openshift.io/plugins: '["logging-view-plugin"]'
    containerImage: quay.io/openshift-logging/cluster-logging-operator:latest
    createdAt: "2018-08-01T08:00:00Z"
    description: The Red Hat OpenShift Logging Operator for OCP provides a means for
      configuring and managing your aggregated logging stack.
    olm.skipRange: '>=4.6.0-0 <5.5.0'
    operatorframework.io/cluster-monitoring: "true"
    operatorframework.io/suggested-namespace: openshift-logging
    operators.openshift.io/infrastructure-features: '["disconnected","proxy-aware"]'
    operators.operatorframework.io/builder: operator-sdk-unknown
    operators.operatorframework.io/project_layout: go.kubebuilder.io/v2
    support: AOS Logging (team-logging@redhat.com)
    target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
  labels:
    operatorframework.io/arch.amd64: supported
    operatorframework.io/arch.arm64: supported
    operatorframework.io/arch.ppc64le: supported
    operatorframework.io/arch.s390x: supported
  name: clusterlogging.v0.0.0
  namespace: placeholder
spec:
  apiservicedefinitions: {}
  customresourcedefinitions:
    owned:
    - description: "ClusterLogForwarder is an API to configure forwarding logs. \n
        You configure forwarding by specifying a list of `pipelines`, which forward
        from a set of named inputs to a set of named outputs. \n There are built-in
        input names for common log categories, and you can define custom inputs to
        do additional filtering. \n There is a built-in output name for the default
        openshift log store, but you can define your own outputs with a URL and other
        connection information to forward logs to other stores or processors, inside
        or outside the cluster. \n For more details see the documentation on the API
        fields."
      displayName: Cluster Log Forwarder
      kind: ClusterLogForwarder
      name: clusterlogforwarders.logging.openshift.io
      specDescriptors:
      - description: "Inputs are named filters for log messages to be forwarded. \n
          There are three built-in inputs named `application`, `infrastructure` and
          `audit`. You don't need to define inputs here if those are sufficient for
          your needs. See `inputRefs` for more."
        displayName: Forwarder Inputs
        path: inputs
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:forwarderInputs
      - description: "Outputs are named destinations for log messages. \n There is
          a built-in output named `default` which forwards to the default openshift
          log store. You can define outputs to forward to other stores or log processors,
          inside or outside the cluster."
        displayName: Forwarder Outputs
        path: outputs
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:forwarderOutputs
      - description: Pipelines forward the messages selected by a set of inputs to
          a set of outputs.
        displayName: Forwarder Pipelines
        path: pipelines
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:forwarderPipelines
      statusDescriptors:
      - description: Conditions of the log forwarder.
        displayName: Forwarder Conditions
        path: conditions
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:forwarderConditions
      - description: Inputs maps input name to condition of the input.
        displayName: Input Conditions
        path: inputs
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:inputConditions
      - description: Outputs maps output name to condition of the output.
        displayName: Output Conditions
        path: outputs
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:outputConditions
      - description: Pipelines maps pipeline name to condition of the pipeline.
        displayName: Pipeline Conditions
        path: pipelines
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:pipelineConditions
      version: v1
    - description: A Red Hat OpenShift Logging instance. ClusterLogging is the Schema
        for the clusterloggings API
      displayName: Cluster Logging
      kind: ClusterLogging
      name: clusterloggings.logging.openshift.io
      resources:
      - kind: ConfigMap
        name: ""
        version: v1
      - kind: CronJob
        name: ""
        version: v1
      - kind: Deployment
        name: ""
        version: v1
      - kind: Pod
        name: ""
        version: v1
      - kind: ReplicaSet
        name: ""
        version: v1
      - kind: Role
        name: ""
        version: v1
      - kind: RoleBinding
        name: ""
        version: v1
      - kind: Route
        name: ""
        version: v1
      - kind: Service
        name: ""
        version: v1
      - kind: ServiceAccount
        name: ""
        version: v1
      - kind: ServiceMonitor
        name: ""
        version: v1
      - kind: persistentvolumeclaims
        name: ""
        version: v1
      specDescriptors:
      - description: Define which Nodes the Pods are scheduled on.
        displayName: Collector Node Selector
        path: collection.logs.nodeSelector
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:selector:core:v1:ConfigMap
      - description: The resource requirements for the collector
        displayName: Collector Resource Requirements
        path: collection.logs.resources
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:resourceRequirements
      - description: Define the tolerations the Pods will accept
        displayName: Collector Pod Tolerations
        path: collection.logs.tolerations
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:selector:core:v1:Toleration
      - description: Define which Nodes the Pods are scheduled on.
        displayName: Collector Node Selector
        path: collection.nodeSelector
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:selector:core:v1:ConfigMap
      - description: The resource requirements for the collector
        displayName: Collector Resource Requirements
        path: collection.resources
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:resourceRequirements
      - description: Define the tolerations the Pods will accept
        displayName: Collector Pod Tolerations
        path: collection.tolerations
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:selector:core:v1:Toleration
      - description: The type of Log Collection to configure
        displayName: Collector Implementation
        path: collection.type
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:select:fluentd
        - urn:alm:descriptor:com.tectonic.ui:select:vector
      - description: Number of nodes to deploy for Elasticsearch
        displayName: Elasticsearch Size
        path: logStore.nodeCount
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:podCount
      - description: Define which Nodes the Pods are scheduled on.
        displayName: Elasticsearch Node Selector
        path: logStore.nodeSelector
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:nodeSelector
      - description: The resource requirements for Elasticsearch
        displayName: Elasticsearch Resource Requirements
        path: logStore.resources
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:resourceRequirements
      - description: Define which Nodes the Pods are scheduled on.
        displayName: Kibana Node Selector
        path: visualization.nodeSelector
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:nodeSelector
      - description: Number of instances to deploy for a Kibana deployment
        displayName: Kibana Size
        path: visualization.replicas
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:podCount
      - description: The resource requirements for Kibana
        displayName: Kibana Resource Requirements
        path: visualization.resources
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:resourceRequirements
      statusDescriptors:
      - displayName: Fluentd status
        path: collection.logs.fluentdStatus.pods
        x-descriptors:
        - urn:alm:descriptor:com.tectonic.ui:podStatuses
      version: v1
  description: |-
    # Red Hat OpenShift Logging
    The Red Hat OpenShift Logging Operator orchestrates and manages the aggregated logging stack as a cluster-wide service.

    ##Features
    * **Create/Destroy**: Launch and create an aggregated logging stack to support the entire OCP cluster.
    * **Simplified Configuration**: Configure your aggregated logging cluster's structure like components and end points easily.

    ## Prerequisites and Requirements
    ### Red Hat OpenShift Logging Namespace
    Cluster logging and the Red Hat OpenShift Logging Operator is only deployable to the **openshift-logging** namespace. This namespace
    must be explicitly created by a cluster administrator (e.g. `oc create ns openshift-logging`). To enable metrics
    service discovery add namespace label `openshift.io/cluster-monitoring: "true"`.

    For additional installation documentation see [Deploying cluster logging](https://docs.openshift.com/container-platform/latest/logging/cluster-logging-deploying.html)
    in the OpenShift product documentation.

    ### Elasticsearch Operator
    The Elasticsearch Operator is responsible for orchestrating and managing cluster logging's Elasticsearch cluster.  This
    operator must be deployed to the global operator group namespace
    ### Memory Considerations
    Elasticsearch is a memory intensive application.  Red Hat OpenShift Logging will specify that each Elasticsearch node needs
    16G of memory for both request and limit unless otherwise defined in the ClusterLogging custom resource. The initial
    set of OCP nodes may not be large enough to support the Elasticsearch cluster.  Additional OCP nodes must be added
    to the OCP cluster if you desire to run with the recommended(or better) memory. Each ES node can operate with a
    lower memory setting though this is not recommended for production deployments.
  displayName: Red Hat OpenShift Logging
  icon:
  - base64data: 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
    mediatype: image/svg+xml
  install:
    spec:
      deployments: null
    strategy: ""
  installModes:
  - supported: true
    type: OwnNamespace
  - supported: true
    type: SingleNamespace
  - supported: false
    type: MultiNamespace
  - supported: false
    type: AllNamespaces
  keywords:
  - elasticsearch
  - kibana
  - fluentd
  - logging
  - aggregated
  - efk
  - vector
  links:
  - name: Elastic
    url: https://www.elastic.co/
  - name: Fluentd
    url: https://www.fluentd.org/
  - name: Documentation
    url: https://github.com/openshift/cluster-logging-operator/blob/master/README.md
  - name: Red Hat OpenShift Logging Operator
    url: https://github.com/openshift/cluster-logging-operator
  minKubeVersion: 1.18.3
  provider:
    name: Red Hat, Inc
  version: 5.5.0
