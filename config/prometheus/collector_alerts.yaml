apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector
  namespace: openshift-logging
spec:
  groups:
  - name: logging_collector.alerts
    rules:
    - alert: CollectorNodeDown
      annotations:
        message: "Prometheus could not scrape {{ $labels.namespace }}/{{ $labels.pod }} collector component for more than 10m."
        summary: "Collector cannot be scraped"
      expr: |
        up{app_kubernetes_io_component = "collector", app_kubernetes_io_part_of = "cluster-logging"} == 0
      for: 10m
      labels:
        service: collector
        severity: critical
    - alert: CollectorHighErrorRate
      annotations:
        message: "{{ $value }}% of records have resulted in an error by {{ $labels.namespace }}/{{ $labels.pod }} collector component."
        summary: "{{ $labels.namespace }}/{{ $labels.pod }} collector component errors are high"
      expr: |
        100 * (
            collector:log_num_errors:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          /
            collector:received_events:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          ) > 0.001
      for: 15m
      labels:
        service: collector
        severity: critical
    - alert: CollectorVeryHighErrorRate
      annotations:
        message: "{{ $value }}% of records have resulted in an error by {{ $labels.namespace }}/{{ $labels.pod }} collector component."
        summary: "{{ $labels.namespace }}/{{ $labels.pod }} collector component errors are very high"
      expr: |
        100 * (
            collector:log_num_errors:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          /
            collector:received_events:sum_rate{app_kubernetes_io_part_of = "cluster-logging"}
          ) > 0.05
      for: 15m
      labels:
        service: collector
        severity: critical
    - alert: ElasticsearchDeprecation
      annotations:
        message: "In Red Hat OpenShift Logging Operator 6.0, support for the Red Hat Elasticsearch Operator has been removed. Bug fixes and support are provided only through the end of the 5.9 lifecycle. As an alternative to the Elasticsearch Operator, you can use the Loki Operator instead."
        summary: "Detected Elasticsearch as the in-cluster storage, which has been removed in 6.0 release"
      expr: |
        sum(kube_pod_labels{namespace="openshift-logging",label_component='elasticsearch'}) > 0
      for: 5m
      labels:
        service: storage
        severity: Warning
        namespace: openshift-logging
    - alert: FluentdDeprecation
      annotations:
        message: "In Red Hat OpenShift Logging Operator 6.0, support for Fluentd as a collector has been removed. Bug fixes and support are provided only through the end of the 5.9 lifecycle. As an alternative to Fluentd, you can use the Vector collector instead."
        summary: "Detected Fluentd as the collector, which has been removed in a 6.0 release"
      expr: |
        sum(kube_pod_labels{namespace="openshift-logging", label_implementation='fluentd', label_app_kubernetes_io_managed_by="cluster-logging-operator"}) > 0
      for: 5m
      labels:
        service: collector
        severity: Warning
        namespace: openshift-logging
    - alert: KibanaDeprecation
      annotations:
        message: "In Red Hat OpenShift Logging Operator 6.0, support for Kibana as a data visualization dashboard has been removed. Bug fixes and support are provided only through the end of the 5.9 lifecycle. As an alternative to Kibana, you can use the Grafana Dashboard instead."
        summary: "Detected Kibana as the log data visualization, which has been removed in the 6.0 release"
      expr: |
        sum(kube_pod_labels{namespace="openshift-logging",label_component='kibana'}) > 0
      for: 5m
      labels:
        service: visualization
        severity: Warning
        namespace: openshift-logging
    - alert: DiskBufferUsage
      annotations:
        message: "Collectors potentially consuming too much node disk, {{ $value }}% "
        summary: "Detected consuming too much node disk on $labels.hostname host"
      expr: |
        (label_replace(sum by(hostname) (vector_buffer_byte_size{component_kind='sink', buffer_type='disk'}), 'instance', '$1', 'hostname', '(.*)') 
        / on(instance) group_left() sum by(instance) (node_filesystem_size_bytes{mountpoint='/var'})) * 100  > 15
      for: 5m
      labels:
        service: collector
        severity: Warning
  - name: logging_clusterlogging_telemetry.rules
    rules:
    - expr: |
        sum by(cluster)(log_logged_bytes_total)
      record: cluster:log_logged_bytes_total:sum
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_instance)(rate(vector_component_errors_total[2m]))
      record: collector:log_num_errors:sum_rate
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_instance)(rate(vector_component_received_events_total[2m]))
      record: collector:received_events:sum_rate




