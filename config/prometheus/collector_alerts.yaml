apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector
  namespace: openshift-logging
spec:
  groups:
  - name: logging_collector.alerts
    rules:
    - alert: ClusterLogForwarderRuntimeConfigurationMissingUnmatched
      annotations:
        message: "The ClusterLogForwarder {{$labels.namespace}}/{{$labels.app_kubernetes_io_instance}} has an incomplete configuration."
        summary: |-
          The ClusterLogForwarder {{$labels.namespace}}/{{$labels.app_kubernetes_io_instance}} has a runtime configuration
          error where routing of unmatched log entries could result in the logs being dropped.  Please open an issue with support
          and include the ClusterLogForwarder and generated collector configuration.
      expr: |
        sum by(namespace,app_kubernetes_io_instance,component_id, log_type, log_source)(irate(logcollector_component_event_unmatched_count[2m])) > 0
      for: 1m
      labels:
        service: collector
        severity: error
    - alert: ClusterLogForwarderOutputErrorRate
      annotations:
        description: |-
          The rate of output errors detected for {{ $labels.namespace }}/{{ $labels.pod }} pod exceeds the threshold of 10%.
        summary: |-
          The pod "{{ $labels.pod }}" owned by ClusterLogForwarder "{{ $labels.namespace }}/{{ $labels.app_kubernetes_io_instance }}" 
          for output "{{ $labels.component_id }}" has been generating the error: "{{ $labels.error_kind }}" for the last 5m
          at the rate of {{ $value | humanizePercentage }} which exceeds the threshold of 10%.
          This could indicate: the output URL is misconfigured, the receiver is unavailable, or there are networking issues for that pod.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-logging-operator/ClusterLogForwarderOutputErrorRate.md
      expr: |
        sum by (namespace,app_kubernetes_io_instance, pod, component_id, error_kind)(irate(vector_http_client_errors_total[5m]) 
        / on (namespace,app_kubernetes_io_instance, pod, component_id) group_left irate(vector_http_client_requests_sent_total[5m])) > 0.10
      for: 5m
      labels:
        service: clusterlogforwarder
        severity: critical
    - alert: ClusterLogForwarderDeprecations
      annotations:
        message: "The Cluster Logging Operator version {{$labels.version}} includes deprecations to some feature of ClusterLogForwarder."
        summary: |-
          The Cluster Logging Operator version {{$labels.version}} includes deprecations to some features of ClusterLogForwarder which 
          will be removed in a future release.  Please see the release notes for details: 
          https://docs.redhat.com/en/documentation/red_hat_openshift_logging/6.4/html/release_notes
      expr: |
        max by (version) (csv_succeeded{exported_namespace="openshift-logging", name=~"cluster-logging.*", version=~"6.4.*"})  > 0
      for: 1m
      labels:
        namespace: openshift-logging
        service: collector
        severity: info
    - alert: CollectorNodeDown
      annotations:
        description: "Prometheus could not scrape {{ $labels.namespace }}/{{ $labels.pod }} collector component for more than 10m."
        summary: "Collector cannot be scraped"
      expr: |
        up{app_kubernetes_io_component = "collector", app_kubernetes_io_part_of = "cluster-logging"} == 0
      for: 10m
      labels:
        service: collector
        severity: critical
    - alert: DiskBufferUsage
      annotations:
        description: "Collectors potentially consuming too much node disk, {{ $value }}% "
        summary: "Detected consuming too much node disk on $labels.hostname host"
      expr: |
        (label_replace(sum by(hostname) (vector_buffer_byte_size{component_kind='sink', buffer_type='disk'}), 'instance', '$1', 'hostname', '(.*)') 
        / on(instance) group_left() sum by(instance) (node_filesystem_size_bytes{mountpoint='/var'})) * 100  > 15
      for: 5m
      labels:
        service: collector
        severity: Warning
    - alert: CollectorHigh403ForbiddenResponseRate
      annotations:
        description: |-
          High rate of "HTTP 403 Forbidden" responses detected for collector "{{ $labels.app_kubernetes_io_instance }}" in namespace {{ $labels.namespace }} for output "{{ $labels.component_id }}". The rate of 403 responses is {{ printf "%.2f" $value }}% over the last 2 minutes, persisting for more than 5 minutes. This could indicate an authorization issue.
        summary: |- 
          At least 10% of sent requests responded with "HTTP 403 Forbidden" for collector "{{ $labels.app_kubernetes_io_instance }}" in namespace {{ $labels.namespace }} for output "{{ $labels.component_id }}"
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-logging-operator/CollectorHigh403ForbiddenResponseRate.md
      expr: |
        sum(
          irate(vector_http_client_responses_total{component_kind="sink", status="403"}[2m])
        ) by (component_id, app_kubernetes_io_instance, namespace)
        /
        sum(
          irate(vector_http_client_responses_total{component_kind="sink"}[2m])
        ) by (component_id, app_kubernetes_io_instance, namespace)
        * 100
        > 10
      for: 5m
      labels:
        service: collector
        severity: critical
  - name: logging_clusterlogging_telemetry.rules
    rules:
    - expr: |
        sum by(cluster)(log_logged_bytes_total)
      record: cluster:log_logged_bytes_total:sum
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_instance)(rate(vector_component_errors_total[2m]))
      record: collector:log_num_errors:sum_rate
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_instance)(rate(vector_component_received_events_total[2m]))
      record: collector:received_events:sum_rate




