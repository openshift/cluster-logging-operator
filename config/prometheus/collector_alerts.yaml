apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector
  namespace: openshift-logging
spec:
  groups:
  - name: logging_collector.alerts
    rules:
    - alert: CollectorNodeDown
      annotations:
        description: "Prometheus could not scrape {{ $labels.namespace }}/{{ $labels.pod }} collector component for more than 10m."
        summary: "Collector cannot be scraped"
      expr: |
        up{app_kubernetes_io_component = "collector", app_kubernetes_io_part_of = "cluster-logging"} == 0
      for: 10m
      labels:
        service: collector
        severity: critical
    - alert: DiskBufferUsage
      annotations:
        description: "Collectors potentially consuming too much node disk, {{ $value }}% "
        summary: "Detected consuming too much node disk on $labels.hostname host"
      expr: |
        (label_replace(sum by(hostname) (vector_buffer_byte_size{component_kind='sink', buffer_type='disk'}), 'instance', '$1', 'hostname', '(.*)') 
        / on(instance) group_left() sum by(instance) (node_filesystem_size_bytes{mountpoint='/var'})) * 100  > 15
      for: 5m
      labels:
        service: collector
        severity: Warning
    - alert: CollectorHigh403ForbiddenResponseRate
      annotations:
        description: |-
          High rate of "HTTP 403 Forbidden" responses detected for collector "{{ $labels.app_kubernetes_io_instance }}" in namespace {{ $labels.namespace }} for output "{{ $labels.component_id }}". The rate of 403 responses is {{ printf "%.2f" $value }}% over the last 2 minutes, persisting for more than 5 minutes. This could indicate an authorization issue.
        summary: |- 
          At least 10% of sent requests responded with "HTTP 403 Forbidden" for collector "{{ $labels.app_kubernetes_io_instance }}" in namespace {{ $labels.namespace }} for output "{{ $labels.component_id }}"
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-logging-operator/CollectorHigh403ForbiddenResponseRate.md
      expr: |
        sum(
          irate(vector_http_client_responses_total{component_kind="sink", status="403"}[2m])
        ) by (component_id, app_kubernetes_io_instance, namespace)
        /
        sum(
          irate(vector_http_client_responses_total{component_kind="sink"}[2m])
        ) by (component_id, app_kubernetes_io_instance, namespace)
        * 100
        > 10
      for: 5m
      labels:
        service: collector
        severity: critical
  - name: logging_clusterlogging_telemetry.rules
    rules:
    - expr: |
        sum by(cluster)(log_logged_bytes_total)
      record: cluster:log_logged_bytes_total:sum
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_instance)(rate(vector_component_errors_total[2m]))
      record: collector:log_num_errors:sum_rate
    - expr: |
        sum by(pod, namespace, app_kubernetes_io_instance)(rate(vector_component_received_events_total[2m]))
      record: collector:received_events:sum_rate




