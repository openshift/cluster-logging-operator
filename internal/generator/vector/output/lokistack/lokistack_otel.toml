[transforms.output_default_lokistack_route]
type = "route"
inputs = ["pipeline_fake"]
route.application = '.log_type == "application"'
route.audit = '.log_type == "audit"'
route.infrastructure = '.log_type == "infrastructure"'


# Route logs separately by log_source
[transforms.output_default_lokistack_application_reroute]
type = "route"
inputs = ["output_default_lokistack_route.application"]
route.container = '.log_source == "container"'

# Normalize container log records to OTLP semantic conventions
[transforms.output_default_lokistack_application_container]
type = "remap"
inputs = ["output_default_lokistack_application_reroute.container"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
resource.attributes = append( resource.attributes,
  [
    {"key": "k8s.pod.name", "value": {"stringValue": .kubernetes.pod_name}},
	{"key": "k8s.pod.uid", "value": {"stringValue": .kubernetes.pod_id}},
    {"key": "k8s.container.name", "value": {"stringValue": .kubernetes.container_name}},
    {"key": "k8s.namespace.name", "value": {"stringValue": .kubernetes.namespace_name}}
  ]
)
if exists(.kubernetes.labels) {for_each(object!(.kubernetes.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "k8s.pod.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Append backward compatibility attributes for container logs
resource.attributes = append( resource.attributes,
	[{"key": "kubernetes.pod_name", "value": {"stringValue": .kubernetes.pod_name}},
	{"key": "kubernetes.container_name", "value": {"stringValue": .kubernetes.container_name}},
	{"key": "kubernetes.namespace_name", "value": {"stringValue": .kubernetes.namespace_name}}]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
r.severityText = .level
# Create body from original message or structured
value = .message
if (value == null) { value = encode_json(.structured) }
r.body = {"stringValue": string!(value)}
r.attributes = append(r.attributes,
  [
	{"key": "log.iostream", "value": {"stringValue": .kubernetes.container_iostream}},
	{"key": "level", "value": {"stringValue": .level}}
  ]
)
  # Openshift and kubernetes objects for grouping containers (dropped before sending)
  o = {
      "log_type": .log_type,
      "log_source": .log_source,
      "cluster_id": .openshift.cluster_id
  }
  .kubernetes = {
      "namespace_name": .kubernetes.namespace_name,
      "pod_name": .kubernetes.pod_name,
      "container_name": .kubernetes.container_name
  }
  . = {
    "openshift": o,
    "kubernetes": .kubernetes,
    "resource": resource,
    "logRecords": r
  }
'''

# Merge container logs and group by namespace, pod and container
[transforms.output_default_lokistack_application_groupby_container]
type = "reduce"
inputs = ["output_default_lokistack_application_container"]
expire_after_ms = 15000
max_events = 250
group_by = [".openshift.cluster_id",".kubernetes.namespace_name",".kubernetes.pod_name",".kubernetes.container_name"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Create new resource object for OTLP JSON payload
[transforms.output_default_lokistack_application_resource_logs]
type = "remap"
inputs = ["output_default_lokistack_application_groupby_container"]
source = '''
  . = {
        "resource": {
           "attributes": .resource.attributes,
        },
        "scopeLogs": [
          {"logRecords": .logRecords}
        ]
      }
'''

[sinks.output_default_lokistack_application]
type = "http"
inputs = ["output_default_lokistack_application_resource_logs"]
uri = "https://logging-loki-gateway-http.openshift-logging.svc:8080/api/logs/v1/application/otlp/v1/logs"
method = "post"
payload_prefix = "{\"resourceLogs\":"
payload_suffix = "}"

[sinks.output_default_lokistack_application.encoding]
codec = "json"
except_fields = ["_internal"]

[sinks.output_default_lokistack_application.tls]
ca_file = "/var/run/ocp-collector/config/openshift-service-ca.crt/ca-bundle.crt"

[sinks.output_default_lokistack_application.auth]
strategy = "bearer"
token = "SECRET[kubernetes_secret.test-sa-token/token]"

# Route logs separately by log_source
[transforms.output_default_lokistack_audit_reroute]
type = "route"
inputs = ["output_default_lokistack_route.audit"]
route.auditd = '.log_source == "auditd"'
route.kubeapi = '.log_source == "kubeAPI"'
route.openshiftapi = '.log_source == "openshiftAPI"'
route.ovn = '.log_source == "ovn"'

# Normalize audit log record to OTLP semantic conventions
[transforms.output_default_lokistack_audit_auditd]
type = "remap"
inputs = ["output_default_lokistack_audit_reroute.auditd"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}
# Fill up auditd logRecord object
if exists(.level) { r.severityText = .level }
kv = parse_key_value!(to_string!(get!(.,["_internal","message"])))
if exists(kv.type) {
    r.attributes = push(r.attributes, {"key": "auditd.type", "value": {"stringValue": kv.type }})
}
if exists(kv.msg) {
    msg_str = ""
    if is_array(kv.msg) {
        msg_str = kv.msg[0]
    } else {
        msg_str = kv.msg
    }
    trimmed = slice!(msg_str, find!(msg_str, "(") + 1, -2)
    parts = split!(trimmed, ":")
    r.attributes = push(r.attributes, {"key": "auditd.sequence", "value": {"stringValue": parts[1] }})
}
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit log kube record to OTLP semantic conventions
[transforms.output_default_lokistack_audit_kubeapi]
type = "remap"
inputs = ["output_default_lokistack_audit_reroute.kubeapi"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}
r.attributes = append(r.attributes,
  [
    {"key": "k8s.audit.event.level", "value": {"stringValue": .level}},
    {"key": "k8s.audit.event.stage", "value": {"stringValue": .stage}},
    {"key": "k8s.audit.event.request.uri", "value": {"stringValue": .requestURI}},
    {"key": "k8s.audit.event.request.verb", "value": {"stringValue": .verb}},
    {"key": "k8s.audit.event.response.code", "value": {"intValue": to_string!(.responseStatus.code)}},
    {"key": "k8s.audit.event.user_agent", "value": {"stringValue": .userAgent}},
    {"key": "k8s.user.username", "value": {"stringValue": .user.username}}
  ]
)
values = []
for_each(array!(.user.groups)) -> |_index,group| {
    .group = group
    values = push(values,{"stringValue": group})
}
r.attributes = push(r.attributes,{"key": "k8s.user.groups", "value": {"arrayValue": {"values": values}}})
if exists(.objectRef) {
  r.attributes = append(r.attributes,[
      {"key": "k8s.audit.event.object_ref.resource", "value": {"stringValue": .objectRef.resource}},
      {"key": "k8s.audit.event.object_ref.name", "value": {"stringValue": .objectRef.name}},
      {"key": "k8s.audit.event.object_ref.namespace", "value": {"stringValue": .objectRef.namespace}},
      {"key": "k8s.audit.event.object_ref.api_version", "value": {"stringValue": .objectRef.apiVersion}},
      {"key": "k8s.audit.event.object_ref.api_group", "value": {"stringValue": .objectRef.apiGroup}}
    ]
  )
}
if exists(.annotations) {for_each(object!(.annotations)) -> |key,value| {
    r.attributes = append(r.attributes,
        [{"key": "k8s.audit.event.annotation." + key, "value": {"stringValue": value}}]
    )
}}
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit openshiftAPI record to OTLP semantic conventions
[transforms.output_default_lokistack_audit_openshiftapi]
type = "remap"
inputs = ["output_default_lokistack_audit_reroute.openshiftapi"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}
r.attributes = append(r.attributes,
  [
    {"key": "k8s.audit.event.level", "value": {"stringValue": .level}},
    {"key": "k8s.audit.event.stage", "value": {"stringValue": .stage}},
    {"key": "k8s.audit.event.request.uri", "value": {"stringValue": .requestURI}},
    {"key": "k8s.audit.event.request.verb", "value": {"stringValue": .verb}},
    {"key": "k8s.audit.event.response.code", "value": {"intValue": to_string!(.responseStatus.code)}},
    {"key": "k8s.audit.event.user_agent", "value": {"stringValue": .userAgent}},
    {"key": "k8s.user.username", "value": {"stringValue": .user.username}}
  ]
)
values = []
for_each(array!(.user.groups)) -> |_index,group| {
    .group = group
    values = push(values,{"stringValue": group})
}
r.attributes = push(r.attributes,{"key": "k8s.user.groups", "value": {"arrayValue": {"values": values}}})
if exists(.objectRef) {
  r.attributes = append(r.attributes,[
      {"key": "k8s.audit.event.object_ref.resource", "value": {"stringValue": .objectRef.resource}},
      {"key": "k8s.audit.event.object_ref.name", "value": {"stringValue": .objectRef.name}},
      {"key": "k8s.audit.event.object_ref.namespace", "value": {"stringValue": .objectRef.namespace}},
      {"key": "k8s.audit.event.object_ref.api_version", "value": {"stringValue": .objectRef.apiVersion}},
      {"key": "k8s.audit.event.object_ref.api_group", "value": {"stringValue": .objectRef.apiGroup}}
    ]
  )
}
if exists(.annotations) {for_each(object!(.annotations)) -> |key,value| {
    r.attributes = append(r.attributes,
        [{"key": "k8s.audit.event.annotation." + key, "value": {"stringValue": value}}]
    )
}}
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit log ovn records to OTLP semantic conventions
[transforms.output_default_lokistack_audit_ovn]
type = "remap"
inputs = ["output_default_lokistack_audit_reroute.ovn"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}
# Fill up OVN logRecord object
if exists(.level) { r.severityText = .level }
ovnTokens = split(to_string!(get!(.,["_internal","message"])),"|")
if 0 < length(ovnTokens) { r.attributes = push(r.attributes, {"key": "k8s.ovn.sequence", "value": {"stringValue": ovnTokens[1] }})}
if 1 < length(ovnTokens) { r.attributes = push(r.attributes, {"key": "k8s.ovn.component", "value": {"stringValue": ovnTokens[2] }})}
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Merge audit api and node logs and group by log_source
[transforms.output_default_lokistack_audit_groupby_source]
type = "reduce"
inputs = ["output_default_lokistack_audit_kubeapi","output_default_lokistack_audit_openshiftapi","output_default_lokistack_audit_ovn"]
expire_after_ms = 15000
max_events = 250
group_by = [".openshift.cluster_id",".openshift.log_type",".openshift.log_source"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Merge auditd host logs and group by hostname
[transforms.output_default_lokistack_audit_groupby_host]
type = "reduce"
inputs = ["output_default_lokistack_audit_auditd"]
expire_after_ms = 15000
max_events = 50
group_by = [".openshift.cluster_id",".openshift.hostname",".openshift.log_type",".openshift.log_source"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Create new resource object for OTLP JSON payload
[transforms.output_default_lokistack_audit_resource_logs]
type = "remap"
inputs = ["output_default_lokistack_audit_groupby_host","output_default_lokistack_audit_groupby_source"]
source = '''
  . = {
        "resource": {
           "attributes": .resource.attributes,
        },
        "scopeLogs": [
          {"logRecords": .logRecords}
        ]
      }
'''

[sinks.output_default_lokistack_audit]
type = "http"
inputs = ["output_default_lokistack_audit_resource_logs"]
uri = "https://logging-loki-gateway-http.openshift-logging.svc:8080/api/logs/v1/audit/otlp/v1/logs"
method = "post"
payload_prefix = "{\"resourceLogs\":"
payload_suffix = "}"

[sinks.output_default_lokistack_audit.encoding]
codec = "json"
except_fields = ["_internal"]

[sinks.output_default_lokistack_audit.tls]
ca_file = "/var/run/ocp-collector/config/openshift-service-ca.crt/ca-bundle.crt"

[sinks.output_default_lokistack_audit.auth]
strategy = "bearer"
token = "SECRET[kubernetes_secret.test-sa-token/token]"

# Route logs separately by log_source
[transforms.output_default_lokistack_infrastructure_reroute]
type = "route"
inputs = ["output_default_lokistack_route.infrastructure"]
route.container = '.log_source == "container"'
route.node = '.log_source == "node"'

# Normalize container log records to OTLP semantic conventions
[transforms.output_default_lokistack_infrastructure_container]
type = "remap"
inputs = ["output_default_lokistack_infrastructure_reroute.container"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
resource.attributes = append( resource.attributes,
  [
    {"key": "k8s.pod.name", "value": {"stringValue": .kubernetes.pod_name}},
	{"key": "k8s.pod.uid", "value": {"stringValue": .kubernetes.pod_id}},
    {"key": "k8s.container.name", "value": {"stringValue": .kubernetes.container_name}},
    {"key": "k8s.namespace.name", "value": {"stringValue": .kubernetes.namespace_name}}
  ]
)
if exists(.kubernetes.labels) {for_each(object!(.kubernetes.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "k8s.pod.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Append backward compatibility attributes for container logs
resource.attributes = append( resource.attributes,
	[{"key": "kubernetes.pod_name", "value": {"stringValue": .kubernetes.pod_name}},
	{"key": "kubernetes.container_name", "value": {"stringValue": .kubernetes.container_name}},
	{"key": "kubernetes.namespace_name", "value": {"stringValue": .kubernetes.namespace_name}}]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
r.severityText = .level
# Create body from original message or structured
value = .message
if (value == null) { value = encode_json(.structured) }
r.body = {"stringValue": string!(value)}
r.attributes = append(r.attributes,
  [
	{"key": "log.iostream", "value": {"stringValue": .kubernetes.container_iostream}},
	{"key": "level", "value": {"stringValue": .level}}
  ]
)
  # Openshift and kubernetes objects for grouping containers (dropped before sending)
  o = {
      "log_type": .log_type,
      "log_source": .log_source,
      "cluster_id": .openshift.cluster_id
  }
  .kubernetes = {
      "namespace_name": .kubernetes.namespace_name,
      "pod_name": .kubernetes.pod_name,
      "container_name": .kubernetes.container_name
  }
  . = {
    "openshift": o,
    "kubernetes": .kubernetes,
    "resource": resource,
    "logRecords": r
  }
'''

# Merge container logs and group by namespace, pod and container
[transforms.output_default_lokistack_infrastructure_groupby_container]
type = "reduce"
inputs = ["output_default_lokistack_infrastructure_container"]
expire_after_ms = 15000
max_events = 250
group_by = [".openshift.cluster_id",".kubernetes.namespace_name",".kubernetes.pod_name",".kubernetes.container_name"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Normalize node log events to OTLP semantic conventions
[transforms.output_default_lokistack_infrastructure_node]
type = "remap"
inputs = ["output_default_lokistack_infrastructure_reroute.node"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
resource.attributes = append(resource.attributes,
  [
	{"key": "process.command_line", "value": {"stringValue": .systemd.t.CMDLINE}},
	{"key": "process.executable.name", "value": {"stringValue": .systemd.t.COMM}},
	{"key": "process.executable.path", "value": {"stringValue": .systemd.t.EXE}},
	{"key": "process.pid", "value": {"stringValue": .systemd.t.PID}},
	{"key": "service.name", "value": {"stringValue": .systemd.t.SYSTEMD_UNIT}}
  ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
r.severityText = .level
# Create body from original message or structured
value = .message
if (value == null) { value = encode_json(.structured) }
r.body = {"stringValue": string!(value)}
r.attributes = append(r.attributes, [{"key": "level", "value": {"stringValue": .level}}])
if exists(.systemd.t) {for_each(object!(.systemd.t)) -> |key,value| {
    r.attributes = append(r.attributes,
        [{"key": "systemd.t." + downcase(key), "value": {"stringValue": value}}]
    )
}}
if exists(.systemd.u) {for_each(object!(.systemd.u)) -> |key,value| {
    r.attributes = append(r.attributes,
        [{"key": "systemd.u." + downcase(key), "value": {"stringValue": value}}]
    )
}}
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Merge auditd host logs and group by hostname
[transforms.output_default_lokistack_infrastructure_groupby_host]
type = "reduce"
inputs = ["output_default_lokistack_infrastructure_node"]
expire_after_ms = 15000
max_events = 50
group_by = [".openshift.cluster_id",".openshift.hostname",".openshift.log_type",".openshift.log_source"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Create new resource object for OTLP JSON payload
[transforms.output_default_lokistack_infrastructure_resource_logs]
type = "remap"
inputs = ["output_default_lokistack_infrastructure_groupby_container","output_default_lokistack_infrastructure_groupby_host"]
source = '''
  . = {
        "resource": {
           "attributes": .resource.attributes,
        },
        "scopeLogs": [
          {"logRecords": .logRecords}
        ]
      }
'''

[sinks.output_default_lokistack_infrastructure]
type = "http"
inputs = ["output_default_lokistack_infrastructure_resource_logs"]
uri = "https://logging-loki-gateway-http.openshift-logging.svc:8080/api/logs/v1/infrastructure/otlp/v1/logs"
method = "post"
payload_prefix = "{\"resourceLogs\":"
payload_suffix = "}"

[sinks.output_default_lokistack_infrastructure.encoding]
codec = "json"
except_fields = ["_internal"]

[sinks.output_default_lokistack_infrastructure.tls]
ca_file = "/var/run/ocp-collector/config/openshift-service-ca.crt/ca-bundle.crt"

[sinks.output_default_lokistack_infrastructure.auth]
strategy = "bearer"
token = "SECRET[kubernetes_secret.test-sa-token/token]"

