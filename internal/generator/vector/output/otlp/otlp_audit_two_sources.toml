# Extract trace context from log messages
[transforms.output_otel_collector_trace_context]
type = "remap"
inputs = ["pipeline_my_pipeline_viaq_0"]
source = '''
# 1. Try to extract trace context from structured log fields
if exists(._internal.structured) {
	if exists(._internal.structured.trace_id) {
		._internal.trace_id = downcase(string!(._internal.structured.trace_id))
	}
	if exists(._internal.structured.span_id) {
		._internal.span_id = downcase(string!(._internal.structured.span_id))
	}
	if exists(._internal.structured.trace_flags) {
		._internal.trace_flags = ._internal.structured.trace_flags
	}
}

# 2. If not structured, try parsing the message as JSON
if !exists(._internal.structured) {
	parsed, err = parse_json(._internal.message)
	if err == null {
		if exists(parsed.trace_id) {
			._internal.trace_id = downcase(string!(parsed.trace_id))
		}
		if exists(parsed.span_id) {
			._internal.span_id = downcase(string!(parsed.span_id))
		}
		if exists(parsed.trace_flags) {
			._internal.trace_flags = parsed.trace_flags
		}
	}
}

# 3. Fall back to regex for any fields still missing
if !exists(._internal.trace_id) {
	parsed, err = parse_regex(._internal.message, r'(?i)(trace_id|traceId|traceID|trace\-id|trace\.id)[=:]\s*["\']?(?<trace_id>[0-9a-f]{32})["\']?')
	if err == null && exists(parsed.trace_id) {
		._internal.trace_id = downcase(string!(parsed.trace_id))
	}
}
if !exists(._internal.span_id) {
	parsed, err = parse_regex(._internal.message, r'(?i)(span_id|spanId|spanID|span\-id|span\.id)[=:]\s*["\']?(?<span_id>[0-9a-f]{16})["\']?')
	if err == null && exists(parsed.span_id) {
		._internal.span_id = downcase(string!(parsed.span_id))
	}
}
if !exists(._internal.trace_flags) {
	parsed, err = parse_regex(._internal.message, r'(?i)(trace_flags|traceFlags|flags|trace\-flags|trace\.flags)[=:]\s*["\']?(?<trace_flags>[0-9a-f]{1,2})["\']?')
	if err == null && exists(parsed.trace_flags) {
		._internal.trace_flags = parsed.trace_flags
	}
}
'''

# Route logs separately by log_source
[transforms.output_otel_collector_reroute]
type = "route"
inputs = ["output_otel_collector_trace_context"]
route.kubeapi = '.log_source == "kubeAPI"'
route.openshiftapi = '.log_source == "openshiftAPI"'

[transforms.output_otel_collector_reroute_unmatched]
inputs = ["output_otel_collector_reroute._unmatched"]
type = "log_to_metric"

[[transforms.output_otel_collector_reroute_unmatched.metrics]]
field = "message"
kind = "incremental"
name = "component_event_unmatched_count"
namespace = "logcollector"
tags = {component_id = "output_otel_collector_reroute", log_source = "{{ log_source }}", log_type = "{{ log_type }}", output_type = "lokistack"}
type = "counter"

# Normalize audit log kube record to OTLP semantic conventions
[transforms.output_otel_collector_kubeapi]
type = "remap"
inputs = ["output_otel_collector_reroute.kubeapi"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit openshiftAPI record to OTLP semantic conventions
[transforms.output_otel_collector_openshiftapi]
type = "remap"
inputs = ["output_otel_collector_reroute.openshiftapi"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Merge audit api and node logs and group by log_source
[transforms.output_otel_collector_groupby_source]
type = "reduce"
inputs = ["output_otel_collector_kubeapi","output_otel_collector_openshiftapi"]
expire_after_ms = 15000
max_events = 1
group_by = [".openshift.cluster_id",".openshift.log_type",".openshift.log_source"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Create new resource object for OTLP JSON payload
[transforms.output_otel_collector_resource_logs]
type = "remap"
inputs = ["output_otel_collector_groupby_source"]
source = '''
  . = {
        "resource": {
           "attributes": .resource.attributes,
        },
        "scopeLogs": [
          {"logRecords": .logRecords}
        ]
      }
'''

[sinks.output_otel_collector]
type = "opentelemetry"
inputs = ["output_otel_collector_resource_logs"]
protocol.uri = "http://localhost:4318/v1/logs"
protocol.type = "http"
protocol.method = "post"
protocol.encoding.codec = "json"
protocol.encoding.except_fields = ["_internal"]
protocol.payload_prefix = "{\"resourceLogs\":"
protocol.payload_suffix = "}"
