# Extract trace context from log messages
[transforms.output_otel_collector_trace_context]
type = "remap"
inputs = ["pipeline_my_pipeline_viaq_0"]
source = '''
# 1. Try to extract trace context from structured log fields
if exists(._internal.structured) {
	if exists(._internal.structured.trace_id) {
		._internal.trace_id = downcase(string!(._internal.structured.trace_id))
	}
	if exists(._internal.structured.span_id) {
		._internal.span_id = downcase(string!(._internal.structured.span_id))
	}
	if exists(._internal.structured.trace_flags) {
		._internal.trace_flags = ._internal.structured.trace_flags
	}
}

# 2. If not structured, try parsing the message as JSON
if !exists(._internal.structured) {
	parsed, err = parse_json(._internal.message)
	if err == null {
		if exists(parsed.trace_id) {
			._internal.trace_id = downcase(string!(parsed.trace_id))
		}
		if exists(parsed.span_id) {
			._internal.span_id = downcase(string!(parsed.span_id))
		}
		if exists(parsed.trace_flags) {
			._internal.trace_flags = parsed.trace_flags
		}
	}
}

# 3. Fall back to regex for any fields still missing
if !exists(._internal.trace_id) {
	parsed, err = parse_regex(._internal.message, r'(?i)(trace_id|traceId|traceID|trace\-id|trace\.id)[=:]\s*["\']?(?<trace_id>[0-9a-f]{32})["\']?')
	if err == null && exists(parsed.trace_id) {
		._internal.trace_id = downcase(string!(parsed.trace_id))
	}
}
if !exists(._internal.span_id) {
	parsed, err = parse_regex(._internal.message, r'(?i)(span_id|spanId|spanID|span\-id|span\.id)[=:]\s*["\']?(?<span_id>[0-9a-f]{16})["\']?')
	if err == null && exists(parsed.span_id) {
		._internal.span_id = downcase(string!(parsed.span_id))
	}
}
if !exists(._internal.trace_flags) {
	parsed, err = parse_regex(._internal.message, r'(?i)(trace_flags|traceFlags|flags|trace\-flags|trace\.flags)[=:]\s*["\']?(?<trace_flags>[0-9a-f]{1,2})["\']?')
	if err == null && exists(parsed.trace_flags) {
		._internal.trace_flags = parsed.trace_flags
	}
}
'''

# Route logs separately by log_source
[transforms.output_otel_collector_reroute]
type = "route"
inputs = ["output_otel_collector_trace_context"]
route.auditd = '.log_source == "auditd"'
route.container = '.log_source == "container"'
route.kubeapi = '.log_source == "kubeAPI"'
route.node = '.log_source == "node"'
route.openshiftapi = '.log_source == "openshiftAPI"'
route.ovn = '.log_source == "ovn"'

[transforms.output_otel_collector_reroute_unmatched]
inputs = ["output_otel_collector_reroute._unmatched"]
type = "log_to_metric"

[[transforms.output_otel_collector_reroute_unmatched.metrics]]
field = "message"
kind = "incremental"
name = "component_event_unmatched_count"
namespace = "logcollector"
tags = {component_id = "output_otel_collector_reroute", log_source = "{{ log_source }}", log_type = "{{ log_type }}", output_type = "lokistack"}
type = "counter"

# Normalize container log records to OTLP semantic conventions
[transforms.output_otel_collector_container]
type = "remap"
inputs = ["output_otel_collector_reroute.container"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
resource.attributes = append( resource.attributes,
  [
    {"key": "k8s.pod.name", "value": {"stringValue": .kubernetes.pod_name}},
	{"key": "k8s.pod.uid", "value": {"stringValue": .kubernetes.pod_id}},
    {"key": "k8s.container.name", "value": {"stringValue": .kubernetes.container_name}},
    {"key": "k8s.namespace.name", "value": {"stringValue": .kubernetes.namespace_name}}
  ]
)
if exists(.kubernetes.labels) {for_each(object!(.kubernetes.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "k8s.pod.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Append backward compatibility attributes for container logs
resource.attributes = append( resource.attributes,
	[{"key": "kubernetes.pod_name", "value": {"stringValue": .kubernetes.pod_name}},
	{"key": "kubernetes.container_name", "value": {"stringValue": .kubernetes.container_name}},
	{"key": "kubernetes.namespace_name", "value": {"stringValue": .kubernetes.namespace_name}}]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
r.severityText = .level
# Create body from original message or structured
value = .message
if (value == null) { value = encode_json(.structured) }
r.body = {"stringValue": string!(value)}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

r.attributes = append(r.attributes,
  [
	{"key": "log.iostream", "value": {"stringValue": .kubernetes.container_iostream}},
	{"key": "level", "value": {"stringValue": .level}}
  ]
)
  # Openshift and kubernetes objects for grouping containers (dropped before sending)
  o = {
      "log_type": .log_type,
      "log_source": .log_source,
      "cluster_id": .openshift.cluster_id
  }
  .kubernetes = {
      "namespace_name": .kubernetes.namespace_name,
      "pod_name": .kubernetes.pod_name,
      "container_name": .kubernetes.container_name
  }
  . = {
    "openshift": o,
    "kubernetes": .kubernetes,
    "resource": resource,
    "logRecords": r
  }
'''

# Merge container logs and group by namespace, pod and container
[transforms.output_otel_collector_groupby_container]
type = "reduce"
inputs = ["output_otel_collector_container"]
expire_after_ms = 15000
max_events = 1
group_by = [".openshift.cluster_id",".kubernetes.namespace_name",".kubernetes.pod_name",".kubernetes.container_name"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Normalize node log events to OTLP semantic conventions
[transforms.output_otel_collector_node]
type = "remap"
inputs = ["output_otel_collector_reroute.node"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
resource.attributes = append(resource.attributes,
  [
	{"key": "process.command_line", "value": {"stringValue": .systemd.t.CMDLINE}},
	{"key": "process.executable.name", "value": {"stringValue": .systemd.t.COMM}},
	{"key": "process.executable.path", "value": {"stringValue": .systemd.t.EXE}},
	{"key": "process.pid", "value": {"stringValue": .systemd.t.PID}},
	{"key": "service.name", "value": {"stringValue": .systemd.t.SYSTEMD_UNIT}}
  ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
r.severityText = .level
# Create body from original message or structured
value = .message
if (value == null) { value = encode_json(.structured) }
r.body = {"stringValue": string!(value)}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

r.attributes = append(r.attributes, [{"key": "level", "value": {"stringValue": .level}}])
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit log record to OTLP semantic conventions
[transforms.output_otel_collector_auditd]
type = "remap"
inputs = ["output_otel_collector_reroute.auditd"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

# Fill up auditd logRecord object
if exists(.level) { r.severityText = .level }
kv = parse_key_value!(to_string!(get!(.,["_internal","message"])))
if exists(kv.type) {
    r.attributes = push(r.attributes, {"key": "auditd.type", "value": {"stringValue": kv.type }})
}
if exists(kv.msg) {
    msg_str = ""
    if is_array(kv.msg) {
        msg_str = kv.msg[0]
    } else {
        msg_str = kv.msg
    }
    trimmed = slice!(msg_str, find!(msg_str, "(") + 1, -2)
    parts = split!(trimmed, ":")
    r.attributes = push(r.attributes, {"key": "log.sequence", "value": {"stringValue": parts[1] }})
}
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit log kube record to OTLP semantic conventions
[transforms.output_otel_collector_kubeapi]
type = "remap"
inputs = ["output_otel_collector_reroute.kubeapi"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit openshiftAPI record to OTLP semantic conventions
[transforms.output_otel_collector_openshiftapi]
type = "remap"
inputs = ["output_otel_collector_reroute.openshiftapi"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Normalize audit log ovn records to OTLP semantic conventions
[transforms.output_otel_collector_ovn]
type = "remap"
inputs = ["output_otel_collector_reroute.ovn"]
source = '''
# Create base resource attributes
resource.attributes = []
resource.attributes = append(resource.attributes,
  [
    {"key": "openshift.cluster.uid", "value": {"stringValue": .openshift.cluster_id}},
    {"key": "openshift.log.source", "value": {"stringValue": .log_source}},
    {"key": "openshift.log.type", "value": {"stringValue": .log_type}},
    {"key": "k8s.node.name", "value": {"stringValue": .hostname}}
  ]
)
if exists(.openshift.labels) {for_each(object!(.openshift.labels)) -> |key,value| {
    resource.attributes = append(resource.attributes,
        [{"key": "openshift.label." + key, "value": {"stringValue": value}}]
    )
}}
# Append backward compatibility attributes
resource.attributes = append( resource.attributes,
	[
      {"key": "log_type", "value": {"stringValue": .log_type}},
      {"key": "log_source", "value": {"stringValue": .log_source}},
      {"key": "openshift.cluster_id", "value": {"stringValue": .openshift.cluster_id}},
      {"key": "kubernetes.host", "value": {"stringValue": .hostname}}
    ]
)
# Create logRecord object
r = {"attributes": []}
r.timeUnixNano = to_string(to_unix_timestamp(parse_timestamp!(.@timestamp, format:"%+"), unit:"nanoseconds"))
r.observedTimeUnixNano = to_string(to_unix_timestamp(now(), unit:"nanoseconds"))
# Create body from internal message
r.body = {"stringValue": to_string!(get!(.,["_internal","message"]))}

# Set trace context fields if any
if exists(._internal.trace_id) {
  r.traceId = ._internal.trace_id
}
if exists(._internal.span_id) {
  r.spanId = ._internal.span_id
}
if exists(._internal.trace_flags) {
  r.flags = ._internal.trace_flags
}

# Fill up OVN logRecord object
if exists(.level) { r.severityText = .level }
ovnTokens = split(to_string!(get!(.,["_internal","message"])),"|")
if 0 < length(ovnTokens) { r.attributes = push(r.attributes, {"key": "log.sequence", "value": {"stringValue": ovnTokens[1] }})}
if 1 < length(ovnTokens) { r.attributes = push(r.attributes, {"key": "k8s.ovn.component", "value": {"stringValue": ovnTokens[2] }})}
# Openshift object for grouping (dropped before sending)
o = {
    "log_type": .log_type,
    "log_source": .log_source,
    "hostname": .hostname,
    "cluster_id": .openshift.cluster_id
}
. = {
  "openshift": o,
  "resource": resource,
  "logRecords": r
}
'''

# Merge audit api and node logs and group by log_source
[transforms.output_otel_collector_groupby_source]
type = "reduce"
inputs = ["output_otel_collector_kubeapi","output_otel_collector_openshiftapi","output_otel_collector_ovn"]
expire_after_ms = 15000
max_events = 1
group_by = [".openshift.cluster_id",".openshift.log_type",".openshift.log_source"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Merge auditd host logs and group by hostname
[transforms.output_otel_collector_groupby_host]
type = "reduce"
inputs = ["output_otel_collector_auditd","output_otel_collector_node"]
expire_after_ms = 15000
max_events = 1
group_by = [".openshift.cluster_id",".openshift.hostname",".openshift.log_type",".openshift.log_source"]
merge_strategies.resource = "retain"
merge_strategies.logRecords = "array"

# Create new resource object for OTLP JSON payload
[transforms.output_otel_collector_resource_logs]
type = "remap"
inputs = ["output_otel_collector_groupby_container","output_otel_collector_groupby_host","output_otel_collector_groupby_source"]
source = '''
  . = {
        "resource": {
           "attributes": .resource.attributes,
        },
        "scopeLogs": [
          {"logRecords": .logRecords}
        ]
      }
'''

[sinks.output_otel_collector]
type = "opentelemetry"
inputs = ["output_otel_collector_resource_logs"]

[sinks.output_otel_collector.protocol]
uri = "http://localhost:4318/v1/logs"
type = "http"
method = "post"
payload_prefix = "{\"resourceLogs\":"
payload_suffix = "}"
compression = "gzip"

[sinks.output_otel_collector.protocol.encoding]
codec = "json"
except_fields = ["_internal"]

[sinks.output_otel_collector.protocol.request]
retry_initial_backoff_secs = 20
retry_max_duration_secs = 35

[sinks.output_otel_collector.batch]
max_bytes = 10000000

[sinks.output_otel_collector.buffer]
type = "disk"
when_full = "block"
max_size = 268435488